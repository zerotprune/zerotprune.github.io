<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="description"
        content="We propose Zero-TPrune, a training-free token pruning framework that leverages the attention graph of pre-trained Transformer models to produce an importance distribution for tokens via our proposed Weighted Page Rank (WPR) algorithm.">
  <meta name="keywords" content="Zero-TPrune, token pruning, training-free, vision transformer, attention-driven">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta property="og:title" content="Zero-TPrune: Zero-Shot Token Pruning through Leveraging of the Attention Graph in Pre-Trained Transformers">
  <meta property="og:description"
        content="We propose Zero-TPrune, a training-free token pruning framework that leverages the attention graph of pre-trained Transformer models to produce an importance distribution for tokens via our proposed Weighted Page Rank (WPR) algorithm.">
  <meta property="og:image" content="https://github.com/zerotprune/zerotprune.github.io/blob/main/static/images/teaser.png">
  <title>Zero-TPrune: Zero-Shot Token Pruning through Leveraging of the Attention Graph in Pre-Trained Transformers</title>

  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=G-PYVRSFMDRL"></script>
  <script>
    window.dataLayer = window.dataLayer || [];

    function gtag() {
      dataLayer.push(arguments);
    }

    gtag('js', new Date());

    gtag('config', 'G-PYVRSFMDRL');
  </script>
  <script type="text/javascript" async
  src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js">
  </script>

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
        rel="stylesheet">

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
        href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">
  <link rel="icon" href="./static/images/Princeton.svg">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/index.js"></script>
</head>
<body>

<!-- <nav class="navbar" role="navigation" aria-label="main navigation">
  <div class="navbar-brand">
    <a role="button" class="navbar-burger" aria-label="menu" aria-expanded="false">
      <span aria-hidden="true"></span>
      <span aria-hidden="true"></span>
      <span aria-hidden="true"></span>
    </a>
  </div>
  <div class="navbar-menu">
    <div class="navbar-start" style="flex-grow: 1; justify-content: center;">
      <a class="navbar-item" href="https://keunhong.com">
      <span class="icon">
          <i class="fas fa-home"></i>
      </span>
      </a>

      <div class="navbar-item has-dropdown is-hoverable">
        <a class="navbar-link">
          More Research
        </a>
        <div class="navbar-dropdown">
          <a class="navbar-item" href="https://hypernerf.github.io">
            HyperNeRF
          </a>
          <a class="navbar-item" href="https://nerfies.github.io">
            Nerfies
          </a>
          <a class="navbar-item" href="https://latentfusion.github.io">
            LatentFusion
          </a>
          <a class="navbar-item" href="https://photoshape.github.io">
            PhotoShape
          </a>
        </div>
      </div>
    </div>

  </div>
</nav> -->


<section class="hero">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column has-text-centered">
          <h1 class="title is-1 publication-title">Zero-TPrune: Zero-Shot Token Pruning through Leveraging of the Attention Graph in Pre-Trained Transformers</h1>
          <div class="is-size-5 publication-authors">
            <span class="author-block">
              <a href="https://hongjiew.github.io/">Hongjie Wang</a>,</span>
            <span class="author-block">
              <a href="https://bhishmadedhia.com/">Bhishma Dedhia</a>,</span>
            <span class="author-block">
              <a href="https://www.princeton.edu/~jha/">Niraj K. Jha</a>,
            </span>
          </div>

          <div class="is-size-5 publication-authors">
            <span class="author-block">Department of Electrical and Computer Engineering, Princeton University</span>
          </div>

          <div class="column has-text-centered">
            <div class="publication-links">
              <!-- PDF Link. -->
              <span class="link-block">
                <a href="https://arxiv.org/abs/2305.17328"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fas fa-file-pdf"></i>
                  </span>
                  <span>Paper</span>
                </a>
              </span>
              <!-- <span class="link-block">
                <a href="https://arxiv.org/abs/2011.12948"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="ai ai-arxiv"></i>
                  </span>
                  <span>Slides</span>
                </a>
              </span> -->
              <!-- Video Link. -->
              <!-- <span class="link-block">
                <a href="https://www.youtube.com/watch?v=MrKrnHhk8IA"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-youtube"></i>
                  </span>
                  <span>Video</span>
                </a>
              </span> -->
              <!-- Code Link. -->
              <span class="link-block">
                <a href="https://github.com/Hongjiew/Zero-TPrune"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-github"></i>
                  </span>
                  <span>Code (coming soon)</span>
                  </a>
              <!-- </span> -->
              <!-- Dataset Link. -->
              <!-- <span class="link-block">
                <a href="https://github.com/google/nerfies/releases/tag/0.1"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="far fa-images"></i>
                  </span>
                  <span>Data</span>
                  </a> -->
            </div>

          </div>
        </div>
      </div>
    </div>
  </div>
</section>

<section class="hero teaser">
  <div class="container is-max-desktop">
    <div class="hero-body">
      <img src="./static/images/teaser.png" 
           alt="Teaser image." 
           class="teaser-image">
      <h2 class="subtitle has-text-centered">
        Zero-TPrune is training-free and can switch between different pruning configurations at no computational cost. 
        This benefits from our graph-based algorithm exploiting correlations between image tokens.
      </h2>
    </div>
  </div>
</section>


<!-- <section class="hero is-light is-small">
  <div class="hero-body">
    <div class="container">
      <div id="results-carousel" class="carousel results-carousel">
        <div class="item item-steve">
          <video poster="" id="steve" autoplay controls muted loop playsinline height="100%">
            <source src="./static/videos/steve.mp4"
                    type="video/mp4">
          </video>
        </div>
        <div class="item item-chair-tp">
          <video poster="" id="chair-tp" autoplay controls muted loop playsinline height="100%">
            <source src="./static/videos/chair-tp.mp4"
                    type="video/mp4">
          </video>
        </div>
        <div class="item item-shiba">
          <video poster="" id="shiba" autoplay controls muted loop playsinline height="100%">
            <source src="./static/videos/shiba.mp4"
                    type="video/mp4">
          </video>
        </div>
        <div class="item item-fullbody">
          <video poster="" id="fullbody" autoplay controls muted loop playsinline height="100%">
            <source src="./static/videos/fullbody.mp4"
                    type="video/mp4">
          </video>
        </div>
        <div class="item item-blueshirt">
          <video poster="" id="blueshirt" autoplay controls muted loop playsinline height="100%">
            <source src="./static/videos/blueshirt.mp4"
                    type="video/mp4">
          </video>
        </div>
        <div class="item item-mask">
          <video poster="" id="mask" autoplay controls muted loop playsinline height="100%">
            <source src="./static/videos/mask.mp4"
                    type="video/mp4">
          </video>
        </div>
        <div class="item item-coffee">
          <video poster="" id="coffee" autoplay controls muted loop playsinline height="100%">
            <source src="./static/videos/coffee.mp4"
                    type="video/mp4">
          </video>
        </div>
        <div class="item item-toby">
          <video poster="" id="toby" autoplay controls muted loop playsinline height="100%">
            <source src="./static/videos/toby2.mp4"
                    type="video/mp4">
          </video>
        </div>
      </div>
    </div>
  </div>
</section> -->


<section class="section">
  <div class="container is-max-desktop">
    <!-- Abstract. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-full-width">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
            Deployment of Transformer models on edge devices is becoming increasingly challenging due to the exponentially growing 
            inference cost that scales quadratically with the number of tokens in the input sequence. Toke pruning is an emerging 
            solution to address this challenge due to its ease of deployment on various Transformer backbones. However, most token 
            pruning methods require computationally expensive fine-tuning, which is undesirable in many edge deployment cases.
          </p>
          <p>
            In this work, we propose <b>Zero-TPrune</b>, the first zero-shot method that considers both the importance and similarity of 
            tokens in performing token pruning. It leverages the attention graph of pre-trained Transformer models to produce an 
            importance distribution for tokens via our proposed Weighted Page Rank (WPR) algorithm. This distribution further guides 
            token partitioning for efficient similarity-based pruning.
          </p>
          <p>
             Due to the elimination of the fine-tuning overhead, Zero-TPrune can prune large models at negligible computational cost, 
             switch between different pruning configurations at no computational cost, and perform hyperparameter tuning efficiently. 
          </p>
          <p>
            We evaluate the performance of Zero-TPrune on vision tasks by applying it to various vision Transformer backbones and 
            testing them on ImageNet. Without any fine-tuning, Zero-TPrune reduces the FLOPs cost of DeiT-S by 34.7% and improves 
            its throughput by 45.3% with only 0.4% accuracy loss. Compared with state-of-the-art pruning methods that require 
            fine-tuning, Zero-TPrune not only eliminates the need for fine-tuning after pruning but also does so with only 0.1% 
            accuracy loss. Compared with state-of-the-art fine-tuning-free pruning methods, Zero-TPrune reduces accuracy loss by 
            up to 49% with the same or higher throughput.
          </p>
        </div>
      </div>
    </div>
    <!--/ Abstract. -->

    <!-- Paper video. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-full-width">
        <h2 class="title is-3">Framework Overview</h2>
        <img src="./static/images/overview.png" 
             alt="Framework overview." 
             class="framework-image">
             <div class="content has-text-justified">
              <p>
                The overall Zero-TPrune framework. Pruning layers can be inserted between Transformer blocks to reduce 
                the number of tokens. Pruning layers comprise <b>I-stage</b> and <b>S-stage</b>: <b>I-stage</b> aims at 
                pruning unimportant tokens of an image, such as background tokens (see (b)); <b>S-stage</b> aims at 
                pruning tokens that are too similar to others, such as repetitive texture tokens (see (c)). 
                A combination of the stages then maximally exploits token redundancy (see (d)).
              </p>
            </div>
      </div>
    </div>
    <!--/ Paper video. -->
  </div>
</section>


<section class="section">
  <div class="container is-max-desktop">

    <div class="columns is-centered has-text-centered">
      <div class="column is-full-width">
        <h2 class="title is-3">Zero-TPrune Shows High Efficiency while Keeping Accuracy </h2>
        <div class="content has-text-justified">
          <p>
            We report the top-1 accuracy, parameters, FLOPs and throughput of the pruned models on ImageNet. We evaluate the models on 224px images unless otherwise noted.
          </p>
        </div>
      </div>
    </div>
      
    <div class="columns is-centered">
      <!-- FID-CLIP Curves. -->
      <!-- <div class="column">
        <div class="content">
          <h2 class="title is-4">Ablation Study</h2>
          <img src="./static/images/ablation.png" 
               alt="Ablation study." 
               class="ablation-image">
          <p>
            Contribution breakdown of the different techniques employed in Zero-TPrune. The backbone model is DeiT-S.
          </p>
        </div>
      </div> -->
      <!--/ FID-CLIP Curves. -->

      <!-- FLOPs Budgets. -->
      <div class="column">
        <h2 class="title is-4">Compare with Fine-Tuning-Required Methods</h2>
        <div class="columns is-centered">
          <div class="column content">
            <img src="./static/images/accu_flops.png" 
                 alt="FLOPs budget." 
                 class="flops-image"
                 width="100%"
                 height="auto">
            <p>
              Performance comparison between Zero-TPrune and state-of-the-art fine-tuning-required methods. The performance
              of fine-tuning-required methods without fine-tuning equals to the performance of randomly pruning tokens.
            </p>
          </div>
        </div>
      </div>
    </div>
    <!--/ FLOPs Budgets. -->

    <div class="columns is-centered">
      <div class="column is-full-width">
        <h2 class="title is-4">Latency Analysis</h2>
        <div class="content has-text-justified">
          <p>
            Comparison between sampling latency in different cases. 
            AT-EDM\(^\dagger\) does not deploying pruning at the second feature level 
            while keeping other pruning rates the same as AT-EDM. Fused Operation (<b>FO</b>): 
            libraries like xformers to boost attention computation. Current Implementation (<b>CI</b>): 
            not provide attention maps as intermediate results. Desired Implementation (<b>DI</b>): provide
            attention maps as intermediate results. 
          </p>
        </div>
      </div>
    </div>

    <div class="columns is-centered">
      <!-- FID-CLIP Curves. -->
      <div class="column">
        <div class="content">
          <img src="./static/images/latency.png" 
               alt="FID-CLIP curves." 
               class="fid-clip-image">
        </div>
      </div>
      <!--/ FID-CLIP Curves. -->

      <!-- FLOPs Budgets. -->
      <div class="column">
        <div class="columns is-centered">
          <div class="column content">
            <img src="./static/images/latency_bd.png" 
                 alt="FLOPs budget." 
                 class="flops-image"
                 width="100%"
                 height="auto">
          </div>
        </div>
      </div>
    </div>
    <!--/ FLOPs Budgets. -->
  </div>
</section>

<section class="section">
  <div class="container is-max-desktop">
    <!-- Animation. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-full-width">
        <h2 class="title is-3">Off-the-Shelf Visual Examples</h2>

        <div class="content has-text-centered">
          <p>
            Visual examples of SD-XL@6.7TFLOPs, ToMe@4.1TFLOPs, and AT-EDM@4.1TFLOPs.
          </p>
        </div>

        <img src="./static/images/example1.png" 
             alt="visual example image." 
             class="example-image">
        
        <h3 class="title is-4"></h3>

        <div class="content has-text-centered">
          <p>
            Visual examples of ToMe and AT-EDM with various FLOPs budgets.
          </p>
        </div>
        
        <img src="./static/images/example2.png"
              alt="visual example image2." 
              class="example-image">

        <!-- Interpolating. -->
        <!-- <h3 class="title is-4">Interpolating states</h3>
        <div class="content has-text-justified">
          <p>
            We can also animate the scene by interpolating the deformation latent codes of two input
            frames. Use the slider here to linearly interpolate between the left frame and the right
            frame.
          </p>
        </div>
        <div class="columns is-vcentered interpolation-panel">
          <div class="column is-3 has-text-centered">
            <img src="./static/images/interpolate_start.jpg"
                 class="interpolation-image"
                 alt="Interpolate start reference image."/>
            <p>Start Frame</p>
          </div>
          <div class="column interpolation-video-column">
            <div id="interpolation-image-wrapper">
              Loading...
            </div>
            <input class="slider is-fullwidth is-large is-info"
                   id="interpolation-slider"
                   step="1" min="0" max="100" value="0" type="range">
          </div>
          <div class="column is-3 has-text-centered">
            <img src="./static/images/interpolate_end.jpg"
                 class="interpolation-image"
                 alt="Interpolation end reference image."/>
            <p class="is-bold">End Frame</p>
          </div>
        </div>
        <br/> -->
        <!--/ Interpolating. -->

        <!-- Re-rendering. -->
        <!-- <h3 class="title is-4">Re-rendering the input video</h3>
        <div class="content has-text-justified">
          <p>
            Using <span class="dnerf">Nerfies</span>, you can re-render a video from a novel
            viewpoint such as a stabilized camera by playing back the training deformations.
          </p>
        </div>
        <div class="content has-text-centered">
          <video id="replay-video"
                 controls
                 muted
                 preload
                 playsinline
                 width="75%">
            <source src="./static/videos/replay.mp4"
                    type="video/mp4">
          </video>
        </div> -->
        <!--/ Re-rendering. -->

      </div>
    </div>
    <!--/ Animation. -->


    <!-- Concurrent Work. -->
    <!-- <div class="columns is-centered">
      <div class="column is-full-width">
        <h2 class="title is-3">Related Links</h2>

        <div class="content has-text-justified">
          <p>
            There's a lot of excellent work that was introduced around the same time as ours.
          </p>
          <p>
            <a href="https://arxiv.org/abs/2104.09125">Progressive Encoding for Neural Optimization</a> introduces an idea similar to our windowed position encoding for coarse-to-fine optimization.
          </p>
          <p>
            <a href="https://www.albertpumarola.com/research/D-NeRF/index.html">D-NeRF</a> and <a href="https://gvv.mpi-inf.mpg.de/projects/nonrigid_nerf/">NR-NeRF</a>
            both use deformation fields to model non-rigid scenes.
          </p>
          <p>
            Some works model videos with a NeRF by directly modulating the density, such as <a href="https://video-nerf.github.io/">Video-NeRF</a>, <a href="https://www.cs.cornell.edu/~zl548/NSFF/">NSFF</a>, and <a href="https://neural-3d-video.github.io/">DyNeRF</a>
          </p>
          <p>
            There are probably many more by the time you are reading this. Check out <a href="https://dellaert.github.io/NeRF/">Frank Dellart's survey on recent NeRF papers</a>, and <a href="https://github.com/yenchenlin/awesome-NeRF">Yen-Chen Lin's curated list of NeRF papers</a>.
          </p>
        </div>
      </div>
    </div> -->
    <!--/ Concurrent Work. -->

  </div>
</section>


<section class="section" id="BibTeX">
  <div class="container is-max-desktop content">
    <h2 class="title">BibTeX</h2>
    <pre><code>@article{wang2023zero,
      title={Zero-TPrune: Zero-Shot Token Pruning through Leveraging of the Attention Graph in Pre-Trained Transformers},
      author={Wang, Hongjie and Dedhia, Bhishma and Jha, Niraj K},
      journal={arXiv preprint arXiv:2305.17328},
      year={2023}
}</code></pre>
  </div>
</section>


<footer class="footer">
  <div class="container">
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">
          <p>
            Website source based on <a href="https://github.com/nerfies/nerfies.github.io">this source code</a>.
          </p>
        </div>
      </div>
    </div>
  </div>
</footer>

</body>
</html>
